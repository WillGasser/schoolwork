{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eLC2fW75sVS"
      },
      "source": [
        "# Will Gasser | Homework 2 | Comp 6630\n",
        "\n",
        "## README\n",
        "required imports:\n",
        "- numpy\n",
        "- matplotlib\n",
        "- time\n",
        "\n",
        "## I had to mount my google drive, my relative file paths will not work for any other user, it is commented out but note the file imports are WRONG for any grader\n",
        "\n",
        "The commentary and questions begin to be answered after I started performing analyses on the model results. Scroll down to see Question 1, just below my activation function tests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdT5PUmu5sVU",
        "outputId": "a7d3e634-65a0-4b8e-8f8e-146626163961"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "#from google.colab import drive #NOTE THIS IS MOUNTED!!!!\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UegBjIy5sVU"
      },
      "source": [
        "## Activation Functions\n",
        "These functions transform the weighted sum of inputs into activations for each neuron. This is just the code implementation for the functions. Once I build our basic perceptron using these functions, I will then comment on the most optimal choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SAagTI-5sVU"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "    s = sigmoid(Z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def linear(Z):\n",
        "    return Z\n",
        "\n",
        "def linear_derivative(Z):\n",
        "    return np.ones_like(Z)\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    return 1 - np.tanh(Z)**2\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return np.where(Z > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPf7PVV5sVV"
      },
      "source": [
        "## Network Initialization\n",
        "These functions initialize our weights and biases and also define our forward pass and loss implementations. Here I wrote the intialize_model function to have a variable amount of neurons in each layer based on our argument inputs. Later I will dicuss how using this approach allowed me to tune my neuron count for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLowDepj5sVV"
      },
      "outputs": [],
      "source": [
        "def initialize_model(input_size, hidden_size, output_size):\n",
        "    \"\"\"Initialize weights and biases for a neural network with one hidden layer\"\"\"\n",
        "    weights1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
        "    bias1 = np.zeros((1, hidden_size))\n",
        "\n",
        "    weights2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
        "    bias2 = np.zeros((1, output_size))\n",
        "\n",
        "    return [weights1, weights2], [bias1, bias2]\n",
        "\n",
        "def forward_pass(X, weights, biases, hidden_activation=sigmoid, output_activation=linear, store_intermediates=False):\n",
        "    Z1 = np.dot(X, weights[0]) + biases[0]\n",
        "    A1 = hidden_activation(Z1)\n",
        "    Z2 = np.dot(A1, weights[1]) + biases[1]\n",
        "    A2 = output_activation(Z2)\n",
        "\n",
        "    if store_intermediates:\n",
        "        return A2, [(Z1, A1), (Z2, A2)]\n",
        "    return A2\n",
        "\n",
        "def compute_mse_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return np.sum((y_pred - y_true) ** 2) / (2 * m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct83RTQD5sVW"
      },
      "source": [
        "## Back Propogation\n",
        "This was the most intensive portion of this exercise. Writing back propogation required me to understand the complexities of the chain rule in relation to adjusting our weights and biases with or previous activations and z's. Once I was able to simplify the notation however, back propogation has a very neat vectorized form. Additionally, this helped bolster my skills in numpy with manipulating matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVwUQFMS5sVW"
      },
      "outputs": [],
      "source": [
        "def backward_pass(X, y, weights, biases, intermediates, hidden_activation_derivative=sigmoid_derivative, output_activation_derivative=linear_derivative):\n",
        "    m = X.shape[0]\n",
        "    Z1, A1 = intermediates[0]\n",
        "    Z2, A2 = intermediates[1]\n",
        "\n",
        "    dZ2 = (A2 - y) * output_activation_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Gradients for hidden layer\n",
        "    dA1 = np.dot(dZ2, weights[1].T)\n",
        "    dZ1 = dA1 * hidden_activation_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return [dW1, dW2], [db1, db2]\n",
        "\n",
        "def update_parameters(weights, biases, weight_grads, bias_grads, learning_rate):\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= learning_rate * weight_grads[i]\n",
        "        biases[i] -= learning_rate * bias_grads[i]\n",
        "    return weights, biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrdSdF1Q5sVW"
      },
      "source": [
        "## Training Function\n",
        "This function includes l2 normalization and early stopping. It also allows us to input our custom activation functions to train our model with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_eYtW3y5sVX"
      },
      "outputs": [],
      "source": [
        "def train_model(X_train, Y_train, X_val=None, Y_val=None, hidden_size=10,\n",
        "                epochs=1000, learning_rate=0.01,\n",
        "                hidden_activation=sigmoid, hidden_activation_derivative=sigmoid_derivative,\n",
        "                output_activation=linear, output_activation_derivative=linear_derivative,\n",
        "                early_stopping=False, patience=10, verbose=True, l2_reg=0.0):\n",
        "\n",
        "    n_samples, input_size = X_train.shape\n",
        "    _, output_size = Y_train.shape if len(Y_train.shape) > 1 else (n_samples, 1)\n",
        "\n",
        "    Y_train = Y_train.reshape(-1, output_size)\n",
        "    if Y_val is not None:\n",
        "        Y_val = Y_val.reshape(-1, output_size)\n",
        "\n",
        "    # Initialize model\n",
        "    weights, biases = initialize_model(input_size, hidden_size, output_size)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_weights = None\n",
        "    best_biases = None\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        y_pred, intermediates = forward_pass(\n",
        "            X_train, weights, biases,\n",
        "            hidden_activation, output_activation,\n",
        "            store_intermediates=True\n",
        "        )\n",
        "        train_loss = compute_mse_loss(y_pred, Y_train)\n",
        "\n",
        "        if l2_reg > 0:\n",
        "            l2_loss = 0\n",
        "            for w in weights:\n",
        "                l2_loss += np.sum(np.square(w))\n",
        "            train_loss += (l2_reg / (2 * n_samples)) * l2_loss\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        weight_grads, bias_grads = backward_pass(\n",
        "            X_train, Y_train, weights, biases, intermediates,\n",
        "            hidden_activation_derivative, output_activation_derivative\n",
        "        )\n",
        "\n",
        "        # L2 regularization for gradients\n",
        "        if l2_reg > 0:\n",
        "            for i in range(len(weights)):\n",
        "                weight_grads[i] += (l2_reg / n_samples) * weights[i]\n",
        "\n",
        "        weights, biases = update_parameters(weights, biases, weight_grads, bias_grads, learning_rate)\n",
        "\n",
        "        if X_val is not None and Y_val is not None:\n",
        "            val_pred = forward_pass(X_val, weights, biases, hidden_activation, output_activation)\n",
        "            val_loss = compute_mse_loss(val_pred, Y_val)\n",
        "\n",
        "            # Add L2 regularization to validation loss if enabled\n",
        "            if l2_reg > 0:\n",
        "                l2_loss = 0\n",
        "                for w in weights:\n",
        "                    l2_loss += np.sum(np.square(w))\n",
        "                val_loss += (l2_reg / (2 * X_val.shape[0])) * l2_loss\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if early_stopping:\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_weights = [w.copy() for w in weights]\n",
        "                    best_biases = [b.copy() for b in biases]\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        if verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                        weights = best_weights\n",
        "                        biases = best_biases\n",
        "                        break\n",
        "\n",
        "        if verbose and (epoch+1) % 100 == 0:\n",
        "            if X_val is not None and Y_val is not None:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_losses[-1]:.6f}, Time: {time.time() - epoch_start:.2f}s\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Time: {time.time() - epoch_start:.2f}s\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Total training time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    if early_stopping and best_weights is not None:\n",
        "        weights = best_weights\n",
        "        biases = best_biases\n",
        "\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses if val_losses else None\n",
        "    }\n",
        "\n",
        "    return weights, biases, history\n",
        "\n",
        "def test_model(X, Y, weights, biases, hidden_activation=sigmoid, output_activation=linear):\n",
        "    Y = Y.reshape(-1, 1) if len(Y.shape) == 1 else Y\n",
        "    predictions = forward_pass(X, weights, biases, hidden_activation, output_activation)\n",
        "    mse = compute_mse_loss(predictions, Y)\n",
        "    return predictions, mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiUW7SZA5sVX"
      },
      "outputs": [],
      "source": [
        "def reshape_data_for_nn(X_train, Y_train, X_test, Y_test):\n",
        "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    Y_train = Y_train.reshape(-1, 1) if len(Y_train.shape) == 1 else Y_train\n",
        "    Y_test = Y_test.reshape(-1, 1) if len(Y_test.shape) == 1 else Y_test\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "def normalize_data(X_train, X_test):\n",
        "    mean = np.mean(X_train, axis=0)\n",
        "    std = np.std(X_train, axis=0)\n",
        "    std[std == 0] = 1\n",
        "\n",
        "    X_train_norm = (X_train - mean) / std\n",
        "    X_test_norm = (X_test - mean) / std\n",
        "\n",
        "    return X_train_norm, X_test_norm, mean, std\n",
        "\n",
        "def split_data(X, Y, validation_ratio=0.2, shuffle=True):\n",
        "    n_samples = X.shape[0]\n",
        "    indices = np.arange(n_samples)\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    val_size = int(n_samples * validation_ratio)\n",
        "    val_indices = indices[:val_size]\n",
        "    train_indices = indices[val_size:]\n",
        "\n",
        "    X_train, X_val = X[train_indices], X[val_indices]\n",
        "    Y_train, Y_val = Y[train_indices], Y[val_indices]\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Tmi8fG5sVY"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curve(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    if history['val_loss'] is not None:\n",
        "        plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(X, Y_true, Y_pred, title=\"Predictions vs True Values\"):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    ax = plt.subplot(2, 1, 1, projection='3d')\n",
        "    ax.scatter(X[:, 0], X[:, 1], Y_true, color='blue', label='True Values')\n",
        "    ax.scatter(X[:, 0], X[:, 1], Y_pred, color='red', alpha=0.5, label='Predictions')\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.set_zlabel('Y')\n",
        "    ax.set_title('3D Visualization of Predictions')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "def plot_learning_rate_curve(learning_rates, losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(learning_rates, losses, marker='o')\n",
        "    plt.xlabel('Learning Rate')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Learning Rate vs MSE Loss')\n",
        "    plt.grid(True)\n",
        "    plt.xscale('log')\n",
        "    plt.show()\n",
        "\n",
        "def plot_neuron_count_curve(neuron_counts, losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(neuron_counts, losses, marker='o')\n",
        "    plt.xlabel('Number of Neurons in Hidden Layer')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Hidden Layer Neurons vs MSE Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_compare_activations(activation_names, losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(activation_names, losses)\n",
        "    plt.xlabel('Activation Function')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Activation Function Comparison')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukOqVr8r5sVY",
        "outputId": "b276a279-894b-4b11-ac31-ecc4994923df"
      },
      "outputs": [],
      "source": [
        "X_train = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/X_train.csv\")\n",
        "Y_train = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/Y_train.csv\")\n",
        "X_test = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/X_test.csv\")\n",
        "Y_test = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/Y_test.csv\")\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"Y_train shape: {Y_train.shape if isinstance(Y_train, np.ndarray) else (len(Y_train),)}\")\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = reshape_data_for_nn(X_train, Y_train, X_test, Y_test)\n",
        "\n",
        "# Normalize features\n",
        "X_train_norm, X_test_norm, mean, std = normalize_data(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aFHv6BL5sVZ"
      },
      "outputs": [],
      "source": [
        "hidden_size = 100 # This is overfitting for sure\n",
        "learning_rate = .001\n",
        "epochs = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhOplL2V5sVZ",
        "outputId": "5246d135-5e3b-4c09-92c7-0e90a30618c0"
      },
      "outputs": [],
      "source": [
        "print(\"Training baseline model with:\")\n",
        "print(f\"Hidden layer neurons: {hidden_size}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "\n",
        "weights, biases, history = train_model(\n",
        "    X_train_norm, Y_train,\n",
        "    hidden_size=hidden_size,\n",
        "    epochs=epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    verbose=True\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFUxSvoe5sVa"
      },
      "source": [
        "# Question 1 & 2\n",
        "\n",
        "1) The linear activation was chosen by the nature of the target variable, which is a continuous, unbounded value, making a linear output more suitable than bounded alternatives like tanh or sigmoid. The linear activation ensures unrestricted output range and facilitates direct error minimization via gradient descent, enhancing predictive accuracy and convergence stability for this regression task.\n",
        "\n",
        "2) The output layer consists of a single neuron, as the predicted variable, Y, is a scalar value representing a singular outcome. The objective of the model is to accurately estimate this individual value. Employing multiple neurons in the output layer would imply a multidimensional output, which exceeds the scope and complexity required for this investigation. Thus, a single output neuron was determined to be the most appropriate configuration for aligning the model’s architecture with the predictive task at hand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "QAXPLsoD5sVa",
        "outputId": "042d26ae-9ec4-4c68-d61c-035fe9bdc201"
      },
      "outputs": [],
      "source": [
        "print(\"Loss curve:\")\n",
        "plot_loss_curve(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adxcjnEk5sVa"
      },
      "source": [
        "# Questions 3 & 4\n",
        "\n",
        "3) The average mean squared error (MSE) loss exhibits significant variation as a function of the number of epochs selected for training the model. The results indicate that with a limited number of epochs, the average MSE remains high, often reaching values in the order of thousands, reflecting poor model convergence. Conversely, when a larger epoch size is employed, the average MSE loss decreases substantially, approaching a minimal value proximate to zero. This observation suggests that extended training iterations enhance the model’s ability to minimize prediction error and achieve optimal performance.\n",
        "\n",
        "4) The graph presented above depicts the loss function, where each epoch represents a complete iteration over the dataset. The function is defined such that f(iteration)=loss, with the MSE loss plotted against the number of epochs. This visualization quantifies the progressive reduction in error as the model iterates through the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zQBy6NiP5sVb",
        "outputId": "fd38be47-ea0e-4923-8798-bfd75c9a04bd"
      },
      "outputs": [],
      "source": [
        "learning_rates = list(np.arange(0.001, .04, .001))\n",
        "lr_mse_results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"  Testing learning rate: {lr}\")\n",
        "    w, b, h = train_model(\n",
        "        X_train_norm, Y_train,\n",
        "        hidden_size=hidden_size,\n",
        "        epochs=100,\n",
        "        learning_rate=lr,\n",
        "        verbose=False,\n",
        "        hidden_activation=relu,\n",
        "        hidden_activation_derivative=relu_derivative,\n",
        "    )\n",
        "\n",
        "    # Test model\n",
        "    _, mse = test_model(X_test_norm, Y_test, w, b, hidden_activation=relu)\n",
        "    lr_mse_results.append(mse)\n",
        "    print(f\"  Learning rate {lr}: MSE = {mse:.6f}\")\n",
        "\n",
        "plot_learning_rate_curve(learning_rates, lr_mse_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fHBHNDD5sVb"
      },
      "source": [
        "# Question 5\n",
        "\n",
        "The ReLu activation function demonstrates heightened sensitivity to the magnitude of the learning rate during model training. Testing revealed that learning rates approaching 1 frequently resulted in numerical instability, manifesting as overflow errors or NaN values, which disrupted convergence. To mitigate these issues, a bounded range of [0.001, 0.04] was selected for the learning rate. Further analysis of the MSE loss across this range indicates that the optimal learning rate for ReLU approximates 0.01, as this value consistently minimized error without compromising numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "hT0YNWiD5sVb",
        "outputId": "5f100bf9-b308-4d20-b204-4cf4b239adee"
      },
      "outputs": [],
      "source": [
        "neuron_counts = list(range(1, 11))\n",
        "neuron_mse_results = []\n",
        "\n",
        "for neuron_count in neuron_counts:\n",
        "    print(f\"  Testing with {neuron_count} neurons in hidden layer\")\n",
        "    w, b, h = train_model(\n",
        "        X_train_norm, Y_train,\n",
        "        hidden_size=neuron_count,\n",
        "        epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        hidden_activation=relu,\n",
        "        hidden_activation_derivative=relu_derivative,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    predictions, mse = test_model(X_test_norm, Y_test, w, b, hidden_activation=relu)\n",
        "    neuron_mse_results.append(mse)\n",
        "\n",
        "\n",
        "plot_neuron_count_curve(neuron_counts, neuron_mse_results)\n",
        "print(f'Average MSE: {np.mean(neuron_mse_results)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05c1aTUD5sVb"
      },
      "source": [
        "# Question 6\n",
        "\n",
        "a. The gradient of the loss function with respect to the parameters did not require re-derivation or modification. This is because the update rule is independent of the network architecture’s scale or number of neurons. The gradients are computed via backpropagation, which adapts automatically to the number of neurons by adjusting the dimensionality of the weight matrices and bias vectors.\n",
        "\n",
        "b. The model’s performance was evaluated by measuring the final mean squared error (MSE) loss and comparing true labels against predicted labels for each configuration (1 to 10 neurons). Experimental results indicate that increasing the number of neurons beyond 3 yields negligible improvements in model performance. With 3 or more neurons, the MSE loss converges to a minimal value, and the predicted labels align closely with the true labels, suggesting the model fully captures the underlying data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JUkaJyj15sVc",
        "outputId": "311b64ef-9fd4-4763-8e27-d29949566aa4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(\"Testing effect of activation functions:\")\n",
        "activation_funcs = {\n",
        "    'sigmoid': (sigmoid, sigmoid_derivative),\n",
        "    'tanh': (tanh, tanh_derivative),\n",
        "    'relu': (relu, relu_derivative)\n",
        "}\n",
        "\n",
        "activation_mse_results = {}\n",
        "\n",
        "for name, (act_func, act_deriv) in activation_funcs.items():\n",
        "    w, b, h = train_model(\n",
        "        X_train_norm, Y_train,\n",
        "        hidden_size=hidden_size,\n",
        "        epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        hidden_activation=act_func,\n",
        "        hidden_activation_derivative=act_deriv,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    predictions, mse = test_model(X_test_norm, Y_test, w, b, hidden_activation=act_func)\n",
        "    activation_mse_results[name] = mse\n",
        "    print(f\"  {name}: MSE = {mse:.6f}\")\n",
        "\n",
        "    plot_predictions(\n",
        "        X_test, Y_test, predictions,\n",
        "        f\"Test Data: True vs Predicted Values ({name} activation)\"\n",
        "    )\n",
        "\n",
        "plot_compare_activations(list(activation_mse_results.keys()), list(activation_mse_results.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqJgVhyj5sVc"
      },
      "source": [
        "# Question 7\n",
        "\n",
        "a. The gradient descent update did not require modification across the tested activation functions. This stability arises because backpropagation computes gradients specific to each activation function’s derivative (e.g., ReLU’s step function, tanh’s hyperbolic tangent derivative), automatically adapting the optimization process without altering the core update mechanism.\n",
        "\n",
        "b. No significant changes were necessary to conduct this experiment, as the network architecture and training pipeline were already coded with flexible activation function support. The existing code allowed seamless substitution of sigmoid, tanh, and ReLU in the hidden layers, requiring only the specification of the desired function prior to training, with all other parameters held constant.\n",
        "\n",
        "c. The experimental outcomes, including final MSE loss and plots of true versus predicted labels, are provided in the graph above. ReLU yielded the lowest MSE, followed by tanh, with sigmoid exhibiting the highest error, indicating ReLU’s superior capacity for error minimization in this context. This ranking reflects ReLU’s ability to mitigate vanishing gradient issues, enhancing convergence compared to the bounded outputs of tanh and sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d3LXJopY5sVc",
        "outputId": "0ab615ad-b5c9-46d8-e4a9-240892ceb26b"
      },
      "outputs": [],
      "source": [
        "hidden_size = 3\n",
        "# Split training data into training and validation sets\n",
        "X_train_split, Y_train_split, X_val, Y_val = split_data(X_train_norm, Y_train, validation_ratio=0.2)\n",
        "\n",
        "patience_values = [5, 10, 20, 50]\n",
        "patience_train_losses = {}\n",
        "patience_val_losses = {}\n",
        "\n",
        "for patience in patience_values:\n",
        "    print(f\"  Testing with patience = {patience}\")\n",
        "\n",
        "    w, b, h = train_model(\n",
        "        X_train_split, Y_train_split,\n",
        "        X_val=X_val, Y_val=Y_val,\n",
        "        hidden_size=hidden_size,\n",
        "        epochs=2000,  # Increase epochs to see early stopping effect\n",
        "        learning_rate=learning_rate,\n",
        "        early_stopping=True,\n",
        "        patience=patience,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    patience_train_losses[patience] = h['train_loss']\n",
        "    patience_val_losses[patience] = h['val_loss']\n",
        "\n",
        "    # Plot train vs validation loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(h['train_loss'], label='Training Loss')\n",
        "    plt.plot(h['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title(f'Train vs Validation Loss (Patience = {patience})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Test model\n",
        "    _, mse = test_model(X_test_norm, Y_test, w, b, hidden_activation=relu)\n",
        "    print(f\"  Patience {patience}: MSE = {mse:.6f}, Stopped at epoch {len(h['train_loss'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z89s-5Yu5sVd"
      },
      "source": [
        "# Question 8\n",
        "\n",
        "This did not function as intended, but my hypothesis to its affect would be:\n",
        "\n",
        "a. Small patience would stop training early, with training loss decreasing slightly and validation loss plateauing higher. Larger patience would extend training, reducing training loss further while validation loss might rise after an initial drop, indicating overfitting.\n",
        "\n",
        "b. Low patience would yield curves with training loss dropping and validation loss flattening early, suggesting underfitting if both remain high. High patience would show training loss nearing zero and validation loss increasing after a minimum, indicating overfitting. Optimal patience would minimize validation loss without significant divergence, balancing fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "GT1nWR8B5sVd",
        "outputId": "2f40b1db-2a0a-4008-e7f9-0045cb473bbd"
      },
      "outputs": [],
      "source": [
        "# Test L2 regularization with different strengths\n",
        "l2_values = [0.0, 0.001, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5]\n",
        "l2_mse_results = []\n",
        "\n",
        "for l2_reg in l2_values:\n",
        "    print(f\"  Testing with L2 regularization = {l2_reg}\")\n",
        "\n",
        "    w, b, h = train_model(\n",
        "        X_train_norm, Y_train,\n",
        "        hidden_size=hidden_size,\n",
        "        epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        l2_reg=l2_reg,\n",
        "        verbose=False,\n",
        "        hidden_activation=relu,\n",
        "        hidden_activation_derivative=relu_derivative,\n",
        "    )\n",
        "\n",
        "    # Test model\n",
        "    _, mse = test_model(X_test_norm, Y_test, w, b, hidden_activation=relu)\n",
        "    l2_mse_results.append(mse)\n",
        "    print(f\"  L2 reg {l2_reg}: MSE = {mse:.6f}\")\n",
        "\n",
        "# Plot L2 regularization vs MSE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(l2_values, l2_mse_results, marker='o')\n",
        "plt.xlabel('L2 Regularization Strength')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('L2 Regularization vs MSE Loss')\n",
        "plt.grid(True)\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzPg3ex15sVd"
      },
      "source": [
        "# Question 9\n",
        "\n",
        "L2 regularization was implemented, adding a penalty term to the loss function, with MSE results depicted in the graph above. MSE decreases rapidly, but then rises sharply, indicating large spikes and suggesting L2 is not consistently accurate across the tested range. Compared to early stopping, which halts training based on validation loss stagnation, L2 directly constrains weights but shows erratic MSE behavior, while early stopping relies on validation tuning and may miss optima. Early stopping is preferred for this dataset, as its adaptive nature better handles the observed instability in L2 regularization performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
